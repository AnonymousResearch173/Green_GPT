{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAdi_ebaiZAM"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-generativeai pandas tqdm\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Enter your API key in place of the ***\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"********************\"\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "# Load your CSV (adjust filename/path)\n",
        "df = pd.read_csv(\"/content/DL_Energy_Patterns.csv\")\n",
        "\n",
        "# Preview relevant columns\n",
        "df = df[['PostId', 'QUESTION', 'CHATGPT_ANSWER', 'SO_ANSWER']]\n",
        "df.head()\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "Objective:\n",
        "You are an expert software sustainability analyst evaluating code solutions for deep learning energy-efficiency. Your task is to analyze two solutions (Human Stack Overflow vs AI ChatGPT) using the established 8 energy-efficiency patterns with numerical scoring.\n",
        "\n",
        "Task Description:\n",
        "Evaluate both code solutions against the 8 core energy-efficiency patterns using a 1-5 scoring system. Provide a concise explanation highlighting key differences and evidence.\n",
        "\n",
        "Scoring Framework (1-5 scale):\n",
        "1: Poor - Pattern not implemented or severely deficient\n",
        "2: Below Average - Basic implementation with significant issues\n",
        "3: Average - Adequate implementation meeting basic requirements\n",
        "4: Good - Solid implementation with clear energy benefits\n",
        "5: Excellent - Exceptional implementation demonstrating best practices\n",
        "\n",
        "Patterns and Associated Tactics:\n",
        "\n",
        "1. PRE-TRAINED MODEL UTILIZATION\n",
        "   - Measurable Tactics: Transfer Learning (T16), Knowledge Distillation (T17)\n",
        "   - Evidence: Pre-trained model loading, fine-tuning, teacher-student architectures\n",
        "\n",
        "2. CHECKPOINT MANAGEMENT\n",
        "   - Measurable Tactics: Checkpoint Usage (T19)\n",
        "   - Evidence: Model saving/loading strategies, resume training capabilities\n",
        "\n",
        "3. MODEL OPTIMIZATION STRATEGIES\n",
        "   - Measurable Tactics: Reduce Complexity (T8), Enhance Sparsity (T14), Energy-aware Pruning (T15)\n",
        "   - Evidence: Simpler architectures, regularization, pruning implementations\n",
        "\n",
        "4. QUANTIZATION TECHNIQUES\n",
        "   - Measurable Tactics: Input Quantization (T4), Quantization-aware Training (T18)\n",
        "   - Evidence: Precision reduction, quantization APIs, model optimization\n",
        "\n",
        "5. EFFICIENT DATA HANDLING\n",
        "   - Measurable Tactics: Sampling (T1), Remove Redundant Data (T2), Feature Reduction (T3), Minimize Data Referencing (T27)\n",
        "   - Evidence: Data pipelines, sampling techniques, feature selection, generators\n",
        "\n",
        "6. MEMORY MANAGEMENT\n",
        "   - Measurable Tactics: Memory Constraints (T20), Computation Partitioning (T22)\n",
        "   - Evidence: Device management, memory optimization, model partitioning\n",
        "\n",
        "7. ALGORITHM & COMPUTATION OPTIMIZATION\n",
        "   - Measurable Tactics: Energy-efficient Algorithms (T6), Lightweight Alternatives (T7), Dynamic Parameter Adaptation (T10), Built-in Library Functions (T11)\n",
        "   - Evidence: Model architecture choices, adaptive learning, optimized operations\n",
        "\n",
        "8. MODEL MAINTENANCE & ADAPTATION\n",
        "   - Measurable Tactics: Graph Substitution (T13), Informed Adaptation (T28), Retrain When Needed (T29)\n",
        "   - Evidence: Computation graph optimization, adaptive strategies\n",
        "\n",
        "Evaluation Process:\n",
        "1. Evaluate each of the 8 patterns independently for both solutions\n",
        "2. Assign scores 1-5 based on implementation quality and evidence\n",
        "3. Consider both explicit code implementations AND textual recommendations\n",
        "4. For Human answers: Give credit for clear textual suggestions even without full code\n",
        "5. For AI answers: Focus on actual code implementations and practical suggestions\n",
        "6. Identify specific tactics (T1-T30) observed in the code or text\n",
        "7. Calculate average scores for each solution\n",
        "8. Determine winner based on higher average score\n",
        "9. Provide concise analysis highlighting key differentiating factors\n",
        "\n",
        "Important Scoring Guidelines:\n",
        "- Code Evidence: Highest priority - explicit implementations get higher scores\n",
        "- Textual Recommendations: Medium priority - clear suggestions without code get moderate scores\n",
        "- Vague Mentions: Low priority - general statements without specifics get low scores\n",
        "- Human Answers: Often contain valuable expert suggestions in natural language\n",
        "- AI Answers: Typically provide complete code implementations\n",
        "\n",
        "Tie Consideration:\n",
        "- Primary determination is based on numerical scores (higher average wins)\n",
        "- Objective tie condition: Only when score difference is exactly 0.0\n",
        "- For minimal differences (≤0.25), carefully evaluate if solutions demonstrate truly complementary strengths that make them equally valuable\n",
        "- Consider tie ONLY when both solutions excel in different but equally important aspects that balance each other\n",
        "- Avoid automatic ties for small differences - the higher score should generally win unless there are compelling complementary factors\n",
        "\n",
        "Reporting Instructions:\n",
        "Return ONLY a JSON object with this exact structure:\n",
        "{{\n",
        "  \"human_pattern_1_score\": 1-5,\n",
        "  \"human_pattern_2_score\": 1-5,\n",
        "  \"human_pattern_3_score\": 1-5,\n",
        "  \"human_pattern_4_score\": 1-5,\n",
        "  \"human_pattern_5_score\": 1-5,\n",
        "  \"human_pattern_6_score\": 1-5,\n",
        "  \"human_pattern_7_score\": 1-5,\n",
        "  \"human_pattern_8_score\": 1-5,\n",
        "\n",
        "  \"ai_pattern_1_score\": 1-5,\n",
        "  \"ai_pattern_2_score\": 1-5,\n",
        "  \"ai_pattern_3_score\": 1-5,\n",
        "  \"ai_pattern_4_score\": 1-5,\n",
        "  \"ai_pattern_5_score\": 1-5,\n",
        "  \"ai_pattern_6_score\": 1-5,\n",
        "  \"ai_pattern_7_score\": 1-5,\n",
        "  \"ai_pattern_8_score\": 1-5,\n",
        "\n",
        "  \"human_average_score\": \"calculated average\",\n",
        "  \"ai_average_score\": \"calculated average\",\n",
        "  \"score_difference\": \"difference in averages\",\n",
        "  \"winner\": \"Human\" or \"AI\" or \"Tie\",\n",
        "  \"analysis_explanation\": \"Brief explanation for your analysis, highlighting key differentiating factors in human and AI solutions, citing relevant code snippets. In case of tie, explain compelling complementary strengths that justify equal evaluation despite numerical differences.\"\n",
        "}}\n",
        "\n",
        "Examples:\n",
        "Example output with justified tie:\n",
        "{{\n",
        "  \"human_pattern_1_score\": 5,\n",
        "  \"human_pattern_2_score\": 3,\n",
        "  \"human_pattern_3_score\": 4,\n",
        "  \"human_pattern_4_score\": 2,\n",
        "  \"human_pattern_5_score\": 4,\n",
        "  \"human_pattern_6_score\": 3,\n",
        "  \"human_pattern_7_score\": 4,\n",
        "  \"human_pattern_8_score\": 3,\n",
        "\n",
        "  \"ai_pattern_1_score\": 3,\n",
        "  \"ai_pattern_2_score\": 4,\n",
        "  \"ai_pattern_3_score\": 3,\n",
        "  \"ai_pattern_4_score\": 5,\n",
        "  \"ai_pattern_5_score\": 3,\n",
        "  \"ai_pattern_6_score\": 4,\n",
        "  \"ai_pattern_7_score\": 3,\n",
        "  \"ai_pattern_8_score\": 5,\n",
        "\n",
        "  \"human_average_score\": 3.5,\n",
        "  \"ai_average_score\": 3.625,\n",
        "  \"score_difference\": 0.125,\n",
        "  \"winner\": \"Tie\",\n",
        "  \"analysis_explanation\": \"Despite AI's slight numerical advantage (0.125), both solutions demonstrate truly complementary and equally valuable approaches. Human excels in Pre-trained Model Utilization (P1: score 5) with expert-level transfer learning strategies and practical fine-tuning guidance that significantly reduces training energy. AI counters with exceptional Quantization Techniques (P4: score 5) and Model Maintenance (P8: score 5) through advanced optimization implementations. The Human solution provides deep domain expertise that could save substantial computational resources in model development, while AI offers robust technical implementations for runtime efficiency. Given that both approaches address fundamentally different but equally critical aspects of the energy-efficiency pipeline, this evaluation results in a tie.\"\n",
        "}}\n",
        "\n",
        "Example output with clear winner despite small difference:\n",
        "{{\n",
        "  \"human_pattern_1_score\": 4,\n",
        "  \"human_pattern_2_score\": 3,\n",
        "  \"human_pattern_3_score\": 2,\n",
        "  \"human_pattern_4_score\": 1,\n",
        "  \"human_pattern_5_score\": 4,\n",
        "  \"human_pattern_6_score\": 3,\n",
        "  \"human_pattern_7_score\": 3,\n",
        "  \"human_pattern_8_score\": 2,\n",
        "\n",
        "  \"ai_pattern_1_score\": 3,\n",
        "  \"ai_pattern_2_score\": 4,\n",
        "  \"ai_pattern_3_score\": 3,\n",
        "  \"ai_pattern_4_score\": 4,\n",
        "  \"ai_pattern_5_score\": 3,\n",
        "  \"ai_pattern_6_score\": 4,\n",
        "  \"ai_pattern_7_score\": 4,\n",
        "  \"ai_pattern_8_score\": 3,\n",
        "\n",
        "  \"human_average_score\": 2.75,\n",
        "  \"ai_average_score\": 3.5,\n",
        "  \"score_difference\": 0.75,\n",
        "  \"winner\": \"AI\",\n",
        "  \"analysis_explanation\": \"Both solutions effectively address the core `sess.run()` bottleneck by fetching entire weight matrices at once, improving Memory Management (P6) by reducing overhead and data transfers (Human: `h1_val = sess.run(weights['h1'])`, AI: `weight_values = sess.run(tensor)`). The Human solution provides a solid Algorithm & Computation Optimization (P7) by fixing the data extraction, but still uses manual loops for writing. The AI solution excels here by implementing `np.savetxt(fp, weight_values)`, leveraging highly optimized built-in libraries for superior computational efficiency and streamlined I/O. AI's more complete and robust approach, including support for multiple layers, provides a clearer best practice for energy-efficient weight serialization.\"\n",
        "}}\n",
        "\n",
        "---\n",
        "Now evaluate the following:\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Human (Stack Overflow) Answer:\n",
        "{human_answer}\n",
        "\n",
        "AI (ChatGPT) Answer:\n",
        "{ai_answer}\n",
        "\"\"\"\n",
        "\n",
        "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "# Update the pattern_columns list for the simplified output structure\n",
        "pattern_columns = []\n",
        "\n",
        "# Add all 8 pattern scores for human and AI\n",
        "for i in range(1, 9):\n",
        "    pattern_columns.append(f'human_pattern_{i}_score')\n",
        "for i in range(1, 9):\n",
        "    pattern_columns.append(f'ai_pattern_{i}_score')\n",
        "\n",
        "# Add summary columns\n",
        "pattern_columns.extend([\n",
        "    'human_average_score', 'ai_average_score', 'score_difference',\n",
        "    'winner', 'analysis_explanation'\n",
        "])\n",
        "\n",
        "# Initialize empty columns\n",
        "for col in pattern_columns:\n",
        "    df[col] = None\n",
        "\n",
        "def parse_json_response(response_text):\n",
        "    \"\"\"Extract JSON from response text and parse it\"\"\"\n",
        "    try:\n",
        "        # Try to find JSON pattern in the response\n",
        "        json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
        "        if json_match:\n",
        "            json_str = json_match.group()\n",
        "            # Parse the JSON string\n",
        "            import json\n",
        "            return json.loads(json_str)\n",
        "        else:\n",
        "            print(\"No JSON found in response\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing JSON: {e}\")\n",
        "        return None\n",
        "\n",
        "# Define checkpoint interval and rate limiting parameters\n",
        "CHECKPOINT_INTERVAL = 20\n",
        "BASE_DELAY = 2  # Base delay between requests in seconds\n",
        "MAX_RETRIES = 3  # Maximum number of retries for failed requests\n",
        "base_output_path = \"/content/gemini_final\"\n",
        "\n",
        "# Create a copy to track processed rows\n",
        "processed_indices = set()\n",
        "\n",
        "def make_api_request_with_retry(prompt, max_retries=MAX_RETRIES):\n",
        "    \"\"\"Make API request with exponential backoff and retries\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            return response, True  # Success\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            print(f\"Attempt {attempt + 1} failed: {error_msg}\")\n",
        "\n",
        "            # Check if it's a rate limit error\n",
        "            if '503' in error_msg or '429' in error_msg or 'quota' in error_msg.lower():\n",
        "                # Exponential backoff with jitter\n",
        "                wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
        "                print(f\"Rate limit hit. Waiting {wait_time:.2f} seconds before retry...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                # For other errors, wait a shorter time\n",
        "                wait_time = BASE_DELAY * (attempt + 1)\n",
        "                print(f\"API error. Waiting {wait_time:.2f} seconds before retry...\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "    return None, False  # All retries failed\n",
        "\n",
        "for idx, row in tqdm(df.iloc[140:].iterrows(), total=len(df)-140):\n",
        "    # Skip if already processed (in case of restart)\n",
        "    if pd.notna(df.at[idx, 'human_pattern_1_score']):\n",
        "        processed_indices.add(idx)\n",
        "        continue\n",
        "\n",
        "    question = str(row['QUESTION'])\n",
        "    human_answer = str(row['SO_ANSWER'])\n",
        "    ai_answer = str(row['CHATGPT_ANSWER'])\n",
        "\n",
        "    prompt = prompt_template.format(\n",
        "        question=question,\n",
        "        human_answer=human_answer,\n",
        "        ai_answer=ai_answer\n",
        "    )\n",
        "\n",
        "    # Make API request with retry logic\n",
        "    response, success = make_api_request_with_retry(prompt)\n",
        "\n",
        "    if success:\n",
        "        json_data = parse_json_response(response.text)\n",
        "\n",
        "        if json_data:\n",
        "            # Update the dataframe with the parsed JSON data\n",
        "            for key, value in json_data.items():\n",
        "                if key in df.columns:\n",
        "                    df.at[idx, key] = value\n",
        "            processed_indices.add(idx)\n",
        "            print(f\"✓ Successfully processed row {idx}\")\n",
        "        else:\n",
        "            print(f\"✗ Failed to parse JSON for row {idx}\")\n",
        "            # Mark as error but continue\n",
        "            df.at[idx, 'analysis_explanation'] = \"ERROR: Failed to parse response\"\n",
        "    else:\n",
        "        print(f\"✗ All retries failed for row {idx}\")\n",
        "        df.at[idx, 'analysis_explanation'] = \"ERROR: API request failed after retries\"\n",
        "\n",
        "    # Add delay between requests to avoid rate limiting\n",
        "    if idx < len(df) - 1:  # Don't delay after the last request\n",
        "        delay = BASE_DELAY + random.uniform(0, 1)  # Add jitter\n",
        "        print(f\"Waiting {delay:.2f} seconds before next request...\")\n",
        "        time.sleep(delay)\n",
        "\n",
        "    # Save checkpoint every CHECKPOINT_INTERVAL rows\n",
        "    if (idx + 1) % CHECKPOINT_INTERVAL == 0:\n",
        "        batch_number = (idx + 1) // CHECKPOINT_INTERVAL\n",
        "        batch_file = f\"{base_output_path}_batch_{batch_number}.csv\"\n",
        "\n",
        "        # Get the current batch (rows 0 to idx)\n",
        "        current_batch = df.iloc[:(idx + 1)]\n",
        "\n",
        "        print(f\"\\nSaving batch {batch_number} (rows 0-{idx}) to {batch_file}...\")\n",
        "        current_batch.to_csv(batch_file, index=False)\n",
        "        print(f\"Batch {batch_number} saved! Contains {len(current_batch)} rows.\")\n",
        "\n",
        "# Final saves after all rows are processed\n",
        "print(\"\\nFinal save...\")\n",
        "\n",
        "# Save the final combined CSV with all rows\n",
        "final_combined_file = f\"{base_output_path}_complete.csv\"\n",
        "df.to_csv(final_combined_file, index=False)\n",
        "print(f\"Complete dataset saved to {final_combined_file}!\")\n",
        "\n",
        "# Also save individual batches for the remaining rows\n",
        "total_rows = len(df)\n",
        "if total_rows % CHECKPOINT_INTERVAL != 0:\n",
        "    final_batch_number = (total_rows // CHECKPOINT_INTERVAL) + 1\n",
        "    final_batch_file = f\"{base_output_path}_batch_{final_batch_number}.csv\"\n",
        "    df.to_csv(final_batch_file, index=False)\n",
        "    print(f\"Final batch {final_batch_number} saved to {final_batch_file}!\")\n",
        "\n",
        "print(f\"Analysis complete! Processed {len(processed_indices)} rows total.\")\n",
        "\n",
        "# Display the structured results\n",
        "print(\"\\nStructured Results:\")\n",
        "print(df[['PostId', 'QUESTION'] + pattern_columns].head())"
      ]
    }
  ]
}