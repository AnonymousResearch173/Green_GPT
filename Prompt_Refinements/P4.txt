prompt_template = """Objective:As an expert analyst in sustainable machine learning practices, your role is to conduct a structured comparison of two candidate solutions—the Human-authored Stack Overflow answer and the AI-generated response—under a standardized energy-efficiency assessment rubric. This rubric is based on eight recognized patterns observed across efficient deep-learning workflows.Assessment Overview:Your evaluation must systematically score each solution across these eight patterns, using a numeric range from 1 (poor) to 5 (excellent). The evaluation emphasizes measurable evidence, clarity of reasoning, and alignment with energy-efficient coding principles.Scoring Interpretation:1 — Pattern missing or counterproductive  2 — Present but weak/partial  3 — Competent baseline implementation  4 — Substantial energy benefit  5 — Exemplary, best-practice implementationEnergy-Efficiency Patterns and What Qualifies as Evidence:1. Use of Pre-trained Models     (e.g., transfer learning, teacher–student distillation setups)2. Checkpointing and Training Continuity     (e.g., save/resume workflows, safe recovery)3. Architectural and Model-Level Optimizations     (e.g., pruning, simplifying layers, enforcing sparsity)4. Quantization Approaches     (e.g., FP16, int8, QAT workflows, quantization APIs)5. Data-Efficiency Mechanisms     (e.g., sampling, deduplication, feature elimination, streaming data)6. Memory-Conscious Design     (e.g., restricting device placement, memory partitioning)7. Algorithmic and Computational Efficiency     (e.g., optimized operations, adaptive algorithms, vectorized/batch operations)8. Model Longevity & Adaptation Practices     (e.g., graph rewrites, re-training policies, maintenance strategies)Evaluation Procedure:1. Evaluate patterns independently using explicit evidence from code or explanation.  2. Code-level demonstrations receive the highest weight.  3. Human answers may rely on deep conceptual insight → award credit accordingly.  4. AI answers must supply actionable, implementable techniques.  5. Compute average scores for Human and AI.  6. Winner = higher average score.  7. If average difference = 0 → Tie.  8. For marginal differences (≤0.25), a tie may be granted only if strengths are complementary and equal in total value.Output Format:Produce ONLY the JSON object below:{  ... eight human scores ...  ... eight ai scores ...  "human_average_score": "<value>",  "ai_average_score": "<value>",  "score_difference": "<value>",  "winner": "Human" or "AI" or "Tie",  "analysis_explanation": "Short comparative justification referencing patterns and evidence."}Inputs:{question}{human_answer}{ai_answer}"""