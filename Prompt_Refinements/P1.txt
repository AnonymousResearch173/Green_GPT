prompt_template = """Objective:Execute a standardized evaluation protocol to judge the energy-efficiency merit of Human vs AI solutions across eight independently scored patterns.Protocol Summary:Each pattern corresponds to a defined optimization domain. You must assign a score between 1 and 5 to both solutions per pattern, based exclusively on observable evidence in implementation or explanation.Scoring Key:1 = Not demonstrated  2 = Minimal demonstration  3 = Adequate, meets baseline  4 = Strong, well-applied  5 = Ideal, maximally energy-efficientOptimization Domains:Pre-trained Model Domain  Evidence: TL, KD, pretrained checkpoints.Checkpoint Domain  Evidence: save/restore integration, fault tolerance.Model Optimization Domain  Evidence: pruning, simplification, sparsity handling.Quantization Domain  Evidence: int8/FP16 conversions, QAT routines.Data Efficiency Domain  Evidence: sampling, deduplication, lightweight preprocessing.Memory Efficiency Domain  Evidence: layout control, on-device policies, workload partitioning.Computation Optimization Domain  Evidence: optimized instructions, vectorization, adaptive logic.Adaptation & Maintenance Domain  Evidence: graph rewrites, dynamic updates, selective retraining.Protocol Rules:Evaluate all patterns independently.Prioritize concrete code over abstract claims.Humans may receive significant credit for expert conceptual clarity.AI must display practical, implementable guidance.Identify which tactics (T1–T30) are supported.Compute Human and AI averages.Winner = higher average.Ties:allowed only if averages equal ORdifference ≤ 0.25 AND both solutions offer equally strong but different strengths.Return Format:ONLY return the JSON structure below:{"human_pattern_1_score": ...,..."ai_pattern_8_score": ...,"human_average_score": "...","ai_average_score": "...","score_difference": "...","winner": "...","analysis_explanation": "..."}Inputs:{question}{human_answer}{ai_answer}"""