prompt_template = """Objective:You are a software sustainability evaluator responsible for scoring two solutions—one written by a Human (Stack Overflow answer) and one generated by AI (ChatGPT)—across eight established deep-learning energy-efficiency dimensions. Your goal is to apply consistent numeric scoring, compare their strengths, and determine which solution is more energy-efficient.Evaluation Task:Assess both solutions using the same 8-pattern evaluation framework. Assign each pattern a numerical score from 1 to 5. Each pattern represents a well-defined energy-efficiency lever supported by measurable tactics. Evidence may be either explicit implementation (highest weight) or strong textual guidance (moderate weight).Scoring Scale (1–5):1 = No implementation or incorrect approach  2 = Weak, partial, or flawed approach  3 = Basic but acceptable implementation  4 = Solid implementation with clear benefits  5 = Best-practice, energy-optimized implementationEnergy-Efficiency Dimensions (Patterns + Observable Tactics):1. Pre-trained Model Utilization     Indicators: Transfer Learning, Distillation, fine-tuning strategies.2. Checkpoint Handling     Indicators: Structured save/load logic, resume-training workflows.3. Model Optimization     Indicators: Complexity reduction, sparsity use, pruning strategies.4. Quantization     Indicators: Precision reduction, QAT, quantization frameworks.5. Efficient Data Usage     Indicators: Sampling, redundant-data removal, feature trimming, generator/pipeline optimization.6. Memory Efficiency     Indicators: Device scoping, controlled allocations, model splitting/partitioning.7. Computation & Algorithm Efficiency     Indicators: Efficient algorithms, adaptive computation, use of optimized built-in operations.8. Model Adaptability & Maintenance     Indicators: Graph rewrites, informed updates, selective retraining.Evaluation Method:1. Score each of the 8 patterns independently for Human and AI.2. Use code evidence as primary signal; textual explanation as secondary.3. Human answers may receive strong credit for expert reasoning even without full code.4. AI answers must demonstrate practical, actionable techniques in code or structured steps.5. Sum the eight scores for each solution and compute the average.6. Determine the winner based on higher average.7. If averages match exactly → Tie.8. If the difference ≤ 0.25, allow the possibility of a tie only when both solutions address different but equally critical efficiency aspects with comparable value.Output Requirements:Return ONLY the following JSON object:{  "human_pattern_1_score": 1-5,  "human_pattern_2_score": 1-5,  "human_pattern_3_score": 1-5,  "human_pattern_4_score": 1-5,  "human_pattern_5_score": 1-5,  "human_pattern_6_score": 1-5,  "human_pattern_7_score": 1-5,  "human_pattern_8_score": 1-5,  "ai_pattern_1_score": 1-5,  "ai_pattern_2_score": 1-5,  "ai_pattern_3_score": 1-5,  "ai_pattern_4_score": 1-5,  "ai_pattern_5_score": 1-5,  "ai_pattern_6_score": 1-5,  "ai_pattern_7_score": 1-5,  "ai_pattern_8_score": 1-5,  "human_average_score": "<computed>",  "ai_average_score": "<computed>",  "score_difference": "<computed>",  "winner": "Human" or "AI" or "Tie",  "analysis_explanation": "Concise analytical justification focusing on differentiating patterns, tactics, and evidence."}Inputs:Question: {question}Human Answer: {human_answer}AI Answer: {ai_answer}"""